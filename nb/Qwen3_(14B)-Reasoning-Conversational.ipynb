{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liuzhongh/Qwen3_-14B--Reasoning-Conversational.ipynb/blob/main/nb/Qwen3_(14B)-Reasoning-Conversational.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSm1usBDEbQ2"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDE6msqvEbQ9"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Lw_AvBBEbQ-"
      },
      "source": [
        "Read our **[Qwen3 Guide](https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune)** and check out our new **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants which outperforms other quantization methods!\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-KUzHL9EbRA"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "W1EsFVj3EbRB"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iajq1W8ipjyK"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572,
          "referenced_widgets": [
            "46186624006a4eea859744048fe594c7",
            "44ac4fb0a3c846d89e996314bfe3e1ae",
            "790b25035fe84b37a9e24c762ebc13d3",
            "937bdb6c4a554cf9b6811c02a304d9f5",
            "877b11937b7f444fbfa43f55ed620fd0",
            "bf9ddee28f274933b80f54b5d1e88b17",
            "28708166d846454b825d52939ae1d724",
            "a89e4d3dfcd644e78564fc296b074ba4",
            "c8632fafb58b432ba073efb61687129f",
            "f701d237a10b4a2f9827bcfd439179f7",
            "1a3d9e56a59a4c8db420852ef5156434",
            "d50917deef714c20b8e158fd0c511b2f",
            "afa887db2ee242ca85eceba01b95f1bb",
            "14ef466c81d2403d85d94edf74dd2d62",
            "893ccc2e66c0460d91c343d582a813f1",
            "7ab25251bde24c86b8999ddeb7516a15",
            "3448b0f4e65445c1a03d408e76c55c23",
            "e214389ab5c04aa08cecdd2936a98a9d",
            "7d82948336734707b62ab52a710abae7",
            "1b054bd50caa4ddbba3b8f000588151f",
            "aa2e792c375f44a9a3a8cf633875c7b7",
            "8a4ede5fb6a148ef8560f5cb49452b81",
            "e36930fd8c3247729f4e5680c994f4af",
            "6fac19064deb425f96851f539dbc6c7c",
            "d36b4c975e5441f2bfbfaa8c51c5ff1e",
            "bbbbaef892184fe18907bf26a22aa128",
            "6bd821920e1045c6994ec2e0cc420ec0",
            "16e8e37ef86a4fbfa3e66ead5e7e2743",
            "4a1f9317f8c04f47974c721eb50c7617",
            "5d32e9d2a1e3425aa7778c59c6494499",
            "24725c2ed5024a98abc3af148ed2bddb",
            "473de742840f404a823eb15abf14e9ed",
            "d2d44b755c364810a38fb7945725bf78",
            "116d1759e8b9498792a0149a72805285",
            "0c1d70e4ba7847e583ce8e94aa966b50",
            "7c0c4cd03538487daaa677111977418c",
            "8f7e06e87525444ca5fc9e762d26032f",
            "3c3fdc7ab9484bb092fabd4eb792678c",
            "699542d9ab9c40caba7209de29242fa5",
            "58fb8ec5b6ff4baf82ea2ddef20747ec",
            "543a669756384c8cbae634b75bf5fe0d",
            "0083da4e9dcc42329501d8e2b1aceb5b",
            "0339406f97ed4d1f8fe71b6ac290cd22",
            "d0592a2accc54e2dbd72333b7da4e7e6",
            "f4f088dae3624047824554d653979353",
            "23fe0c7169f24c5caab5143d6fa69d75",
            "6dcd56c455284a7e92e6711325e7e548",
            "2d57ab88bfd347a9a9d81283d2ab4329",
            "3001d0f0683f46d4b61a13ab664f292e",
            "e10dd04886414be391f25935d5c2442b",
            "ecbe4e1c2e75487986d055b86e8be38d",
            "b0a7f1496755410aa2adbb8b75b71523",
            "37b2826eeeef494e9574b0ed86a7088d",
            "5ef35248a74f48b7acc9655b6ad53987",
            "23fbfa4e3f4c46f395a1f75cb91da76f",
            "8abbec0a97cd4c5a814d9fd0fce0ef1e",
            "16e623400365499cb05676017a16750d",
            "87d6fc0679604f9b92c37b0ac1958797",
            "8375bd179db2491c915a1d6c222129d8",
            "6bc3ea39056f41fb9cc6f400c959e306",
            "ca4e0020393843828070153d6b5ead62",
            "911ff29273ac4385b403e338f8ecd7bc",
            "84e83551fa87410ea5bd6959ddc0105e",
            "f1104f2268f94a12bc0b809dc9232022",
            "8d26fd54de9f4ee99ffcaa388a9f2d21",
            "cdf75556092d49f6b176bbdc51c631f3",
            "9c7ab5ff8070452297d3d020214c4772",
            "acc3028dbfe04b20bdbdfd59a1c297f7",
            "ff01186bef084d2893afe4de93de38b6",
            "ae6188ebbafd4f26b534dfe0be64a42f",
            "2e221e847fe24fdaa6799577ca555fc8",
            "b447d0dc3b3048c699bea6ae704eb55c",
            "a5a91abe0d1442c58e417fc5def08f4a",
            "d0ccc07defc449729ccc81bf15839945",
            "32a1f50060d546bdb43577d6a48c2d24",
            "4db5a9b277944d83bf890193608fc086",
            "848ff1dd604b43999d3bf24f579de5e1",
            "a45d8f24d7e34d88898edd3bbac0591f",
            "beee83ee2e00434c9c8100aec98ae11f",
            "fa5f722a2ae84bb4adcc85ebc7e90bbf",
            "4a5e0d1757704226b921a788813b588f",
            "0b0324bde1ec4b7dbed795148c03a26b",
            "4d17e6be99ce49448cc31fab1506b6c3",
            "3c1d2f23c28c41d8920254bd27c43eff",
            "605715fc31284d5b8d3dd2124ecd6e38",
            "8f3ed1faaade4beba62a80538e45642c",
            "e1c7dc2dae9a413981c149741461f437",
            "b4b69aeb993541c098012dc2cefa315c",
            "834aa0e6cd5847e285118ae268703223",
            "0a992f8240e04653ba79bc1b0d55e678",
            "7e7381556e6f41c9bed92de81abef1af",
            "ef3ab502906f40fdaf3b114ad899bc27",
            "bc85283ef2b0403c899456a9ee75b504",
            "6a46c8e0d57444879db18e40f3e77f7f",
            "271011d140414927bce6aaa7f24b4d61",
            "7b1c1c1d31d845c99c08dc541ea91e95",
            "ed19b4305d7947dd97451a9cd9d0e885",
            "393d0957fd024fd5b2831ad3442b6786",
            "5b7d5e3fa2e34f81869bdfe6f3b59abb",
            "6ecaa7b8872e443ab62f8af2f5653044",
            "ee238e6b2fb34885a39c7ad73ccf7396",
            "fad9b98c755d48a587f9469740212ec5",
            "25c900d5ac1b42568117dd1e52dce76d",
            "df5532583aa143be86ad181064e9cf88",
            "5ac01f5f911449a9b49ada34accb3b67",
            "ef0a20c2c9ed4028ba10b8f60fde4da1",
            "75f4deb024314549a3fb203db264a5cf",
            "b5b851101ed74034bdd0a1a3d055e12a",
            "039587f25f424968a8f6ef9b841196b9",
            "3a4ef0a6864f47c08a64e7165f87845a",
            "24128691c6194fd6b79d158b7071b326",
            "c8d89290820c46678e26398b8e2ed480",
            "2a82d2f9e11245b28c212c6e4d340556",
            "c370b90974b749d996aae6fc989f486d",
            "a7bebad29118429192c3f237b84d6de0",
            "c6b5615f4f6f4156afd259ec42b53b61",
            "05d0ac6a254b44b786077a7bb1f945a2",
            "b8ff1fc3f68d44fc8fda5c09cd4c38bd",
            "6210ca5701194f2db65ac986eeb9d545",
            "33fe7775022843b585e661f6c3588cf4",
            "c814c994d544475693804f8950e03e44",
            "60993a66bb7a411d94e4d5a0ca4089cc",
            "6b5a3bd7f6c04d51b8731978a6531be2",
            "a299edda6a75461d8262ad4861793366",
            "2b83455920ec4ea38994c081b1a05a6e",
            "2ceb8d8f442643a38417b3380ee8404c",
            "781ba952b1ef4252aa976a274262acf1",
            "1b521565c3e746deba2fc689874bbd05",
            "b141d5cb922a45fcaff46d009e40b2ed",
            "b5eaad523985414a8c38520f0c8fc5f5",
            "93a30a54c6614bb698c3512b54bc92eb",
            "46c8b0a25d944b43a7c5dfbcd3e398c3",
            "3dd52c47a7714aca94599db9db205acf",
            "c456ef6cf84f46f883ad6e7fe5638319",
            "39eee414e1684dc6b4344500c90d7855",
            "64819c3a76fd4475b9c80e9785b05c36",
            "06e6e7cb4a8748f89bb521c3cfbdfb74",
            "2a491546b1b04f7d9e5a6aa5a198d53e",
            "b3d62821d1f647e88d1c609712466edd",
            "2cbe5c780a044af58596a372eac47335",
            "7dfa40186ebe440292c69da055ed458c",
            "59e06e0882494436b2255e0e68f66b3e",
            "06d1391629444a9d95f930fb3fe8548a"
          ]
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "2ba258d9-b1f7-4a88-8596-88fcf693c0db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.4.7: Fast Qwen3 patching. Transformers: 4.51.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/168k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46186624006a4eea859744048fe594c7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d50917deef714c20b8e158fd0c511b2f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/4.59G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e36930fd8c3247729f4e5680c994f4af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/1.56G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "116d1759e8b9498792a0149a72805285"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4f088dae3624047824554d653979353"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8abbec0a97cd4c5a814d9fd0fce0ef1e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/10.3k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c7ab5ff8070452297d3d020214c4772"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a45d8f24d7e34d88898edd3bbac0591f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "834aa0e6cd5847e285118ae268703223"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ecaa7b8872e443ab62f8af2f5653044"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24128691c6194fd6b79d158b7071b326"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "60993a66bb7a411d94e4d5a0ca4089cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "chat_template.jinja:   0%|          | 0.00/4.67k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3dd52c47a7714aca94599db9db205acf"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "fourbit_models = [\n",
        "    \"unsloth/Qwen3-1.7B-unsloth-bnb-4bit\", # Qwen 14B 2x faster\n",
        "    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-14B-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-32B-unsloth-bnb-4bit\",\n",
        "\n",
        "    # 4bit dynamic quants for superior accuracy and low memory use\n",
        "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Phi-4\",\n",
        "    \"unsloth/Llama-3.1-8B\",\n",
        "    \"unsloth/Llama-3.2-3B\",\n",
        "    \"unsloth/orpheus-3b-0.1-ft-unsloth-bnb-4bit\" # [NEW] We support TTS models!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen3-14B\",\n",
        "    max_seq_length = 2048,   # Context length - can be longer, but uses more memory\n",
        "    load_in_4bit = True,     # 4bit uses much less memory\n",
        "    load_in_8bit = False,    # A bit more accurate, uses 2x memory\n",
        "    full_finetuning = False, # We have full finetuning now!\n",
        "    # token = \"hf_...\",      # use one if using gated models\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "cfa21c27-5478-46fa-a3a1-8cc07ce1e60d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.4.7 patched 40 layers with 40 QKV layers, 40 O layers and 40 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,           # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 32,  # Best to choose alpha = rank or rank*2\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,   # We support rank stabilized LoRA\n",
        "    loftq_config = None,  # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "Qwen3 has both reasoning and a non reasoning mode. So, we should use 2 datasets:\n",
        "\n",
        "1. We use the [Open Math Reasoning]() dataset which was used to win the [AIMO](https://www.kaggle.com/competitions/ai-mathematical-olympiad-progress-prize-2/leaderboard) (AI Mathematical Olympiad - Progress Prize 2) challenge! We sample 10% of verifiable reasoning traces that used DeepSeek R1, and whicht got > 95% accuracy.\n",
        "\n",
        "2. We also leverage [Maxime Labonne's FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k) dataset in ShareGPT style. But we need to convert it to HuggingFace's normal multiturn format as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5kyTw2n1edte",
        "outputId": "7c1d1ade-70fe-49ae-d2c2-2cad5e92be46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337,
          "referenced_widgets": [
            "84b44efef4e64c38a99982277885be5e",
            "1a604b625c464c4b8bcd72324874a15d",
            "7383213da54a46c1a7dcf88c1e85dfce",
            "6332d55f4fa24e7aad1fd7e32688c394",
            "97eddd1ef8a84bb0b087fc87ebddd6ef",
            "3d5edf04615e4f7a952a4c957ea0474a",
            "eb681a5c4a5046eda1c53de71f0880d4",
            "7f90f40a5ef1467aab3a7e51fb27822c",
            "118f6e0724b642549e9cf6897aa595f3",
            "916f6273c86b4dbd931a822ae93c3959",
            "be2cc0329d7040938e1a5f36987c4154",
            "973e49f9b4d44032988680b4e68c8306",
            "3bc92e37cf6c4d11b9758df1684e00a8",
            "7ed68089690f4c2da531e23a9cda34a7",
            "632a94d2e2434457a656750302690a92",
            "0b6bb58c939644aca34658568d1466c6",
            "d46c0f22e77741f6975c63de8213140d",
            "a4218a86e87348b786fde260684ba7ed",
            "496eb20490244b64ae5523835848c4c0",
            "90c3e63f60014ff49262c4fee40bda2a",
            "6963dea6e2204e58aebae6b2643059fb",
            "826801fb8af8482c9d76dfb417e16d19",
            "55aa62240d6e4912bac88a9bf832964c",
            "1e336ef9785b4a4cbd6d306486deb727",
            "8ae977ff36034efdb9a2f1dfdf213d8a",
            "301771eb92674573be00082d34b03b35",
            "933cf88af75d4078b6a233a173389316",
            "adb2f7198848457f9e3692b337e6b95a",
            "751e4269521d44aabb434b422c6eb001",
            "4ded23d57b5b45d7ad72c71c5237bfff",
            "71745f77b5664684bba9a258e458f360",
            "a85a367653114ecca1ab0bf5ac91499f",
            "067e92fedf6e472e906a494ba77833bf",
            "91c0956ed0f24666b6ce0be6953e57b7",
            "09e4ddf813464991b8f72927bd7c84a8",
            "a40e8f31d33949a58ad31a25bc08a370",
            "db9cd5a59d0a4b7a89b6b7c3cb9badad",
            "031dafdcf9e94413bf7d887024bdefc9",
            "ffcca9148f774ad99ce8b7a777815a18",
            "beb945d03ac743a282447729682b9da7",
            "c1a579a10cc04f3b90448eda52d373a5",
            "e9ab2c72056842a9b3f2598f3b03db0d",
            "5e5f441b89894c66af3bfae3ac38f3ac",
            "06ec92e299a548be882a909c901b7966",
            "9023014778a54637a470792e42866c61",
            "68e512fe2c524018a4a56ea58075a428",
            "4ee4e29598b0403abbad0449f9759784",
            "973c490016b94f968cadd8d01aaf166e",
            "083daa32aa6f453a9eee03f9b2f27863",
            "693cadaa8d9545a6b43ca2128eea138a",
            "d3d2130c21a8483e9ac0b15387e1b438",
            "5e56e7251414409682ad44a16b4e5fa5",
            "b80d92a4f495426f95d12866605ddd21",
            "82dc940a77e94ea4aaf82bb9c4ea511f",
            "a078faa8c2f24a4681899093189bc863",
            "674c22127aa04bd5a1c3055f6411a728",
            "4e629c1955f743a999d95d5ca7e31fc9",
            "ab38fca44689457fb64ae3c1f1570d4e",
            "57cc9edf1bee46f2ab1f3e4c66f39bae",
            "acc95f4a2e3946a1858a9b7707c24f3d",
            "ea0d4724bf744828a5a734c0b5ad9259",
            "30c39e23b6574291ba1f396dab6a80b6",
            "e9a617d7969f44d892ea82afca667f5e",
            "abfb6233628c4edd957aa4a73f0171bc",
            "6103d18b47d64820bea51871f1884769",
            "43f73d21648d4f8a88fbe7c6197d2716",
            "c0ae6ae4ac62471e9237d16d631debde",
            "50f83c43e52045e2ab4b6306f353b8e9",
            "ff750cb862094b0d8b342899a842d233",
            "3020ab37b2824fce85d40185e0b3ea82",
            "cfda6807ed9b48c888ce7fbba46fd2ec",
            "69040ace338e40d98462d6be12cd0575",
            "9c7606d54aa7486cb26a5e72e2b81497",
            "a20729465f0042739039f8f47d93b3ca",
            "ce4f4b3cfda84a87b6dd967bf2b229e6",
            "8fd6a77b591a4da8be5bbc338d5e609e",
            "f7f9e11dd50c45e2b77dd5e5122846d6",
            "793a4043b1244f31bef56ceb6bc67ccd",
            "88cb3e95166d4e02b39a156d4212dbc1",
            "e96f468e08b44a61a4addb4fbed10464",
            "dd400ed988734f55b69727f0e1983ceb",
            "de119af026ca4eb09b246b774deeaa5e",
            "6db283a5a55c40e2bd12d9574d0561ce",
            "339a5399a3d0403b8af9c05fa49c10e3",
            "d001052caa9f4b2ab392b3896df6419d",
            "ee003897f0aa482d89fb68d31df87f90",
            "3f321bf6d4f146f3a2af4a9605f0842c",
            "7ac3755b07ae4e11aed712e3071506c1",
            "320230c4bb8d420da56146af993ff6d9",
            "9a09f168da6a48c4bd9b810c68248eb4",
            "bda8484c1f2c46798be5a332cb796beb",
            "e210913296e64d54b4eed20a97accb25",
            "f6aab5cf89be4ee4b179379a71ce9a27",
            "1343f59de6924891bd23d5935216d027",
            "22669c879f564a3b8d8f008e4c886bf2",
            "9736b32b3d134c75bda124265600afaf",
            "cc40ff472372414cbdae89a235678861",
            "5169bbd0f133460698d9cfcc6f6d4b93",
            "f036e5ec44354ebba6cf1de9d4267073",
            "fa9cd56206824436be57d9bf37fc1ee1",
            "b122d7fa88734c3a9e0da8179e9a12be",
            "d83c98cfc128485692499f01d0a3232d",
            "b5a2b2acb39b4973959b6ae144e6b71e",
            "43f713c8d7c14bc2ba6a5ed738dac709",
            "ae948221f0554dbcbff9fd6e7c079a87",
            "09235747f9cd455099452efbca43b9aa",
            "cd75c2ef90d24b938d4e94b2334d194c",
            "ee8ade6daa064ba1baff73dfc3eb5a28",
            "3ac2a3b2f61d4cfaa4291b34cc8cd1cf",
            "c4c56d6045f144ab886daedcaa4b1e15"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/19.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "84b44efef4e64c38a99982277885be5e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "SFT_medicalKnowledge_source1_548404.json:   0%|          | 0.00/152M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "973e49f9b4d44032988680b4e68c8306"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "SFT_medicalKnowledge_source2_99334.json:   0%|          | 0.00/23.0M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55aa62240d6e4912bac88a9bf832964c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "SFT_medicalKnowledge_source3_556540.json:   0%|          | 0.00/110M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91c0956ed0f24666b6ce0be6953e57b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "SFT_nlpDiseaseDiagnosed_61486.json:   0%|          | 0.00/109M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9023014778a54637a470792e42866c61"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "SFT_nlpSyndromeDiagnosed_48665.json:   0%|          | 0.00/43.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "674c22127aa04bd5a1c3055f6411a728"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "SFT_structGeneral_310860.json:   0%|          | 0.00/267M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0ae6ae4ac62471e9237d16d631debde"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "SFT_structPrescription_92896.json:   0%|          | 0.00/42.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "793a4043b1244f31bef56ceb6bc67ccd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "_SFT_traditionalTrans_1959542.json:   0%|          | 0.00/683M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "320230c4bb8d420da56146af993ff6d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/3677727 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa9cd56206824436be57d9bf37fc1ee1"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "non_reasoning_dataset = load_dataset(\"SylvanL/Traditional-Chinese-Medicine-Dataset-SFT\", split = \"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTZICZtie3lQ"
      },
      "source": [
        "Let's see the structure of both datasets:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ÈªòËÆ§Ê†áÈ¢òÊñáÊú¨\n",
        "from unsloth.chat_templates import CHAT_TEMPLATES\n",
        "print(list(CHAT_TEMPLATES.keys()))"
      ],
      "metadata": {
        "id": "5njoKV1dPyUB",
        "outputId": "bc8b4db9-b472-4b88-d801-86ade55f8bf3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['unsloth', 'zephyr', 'chatml', 'mistral', 'llama', 'vicuna', 'vicuna_old', 'vicuna old', 'alpaca', 'gemma', 'gemma_chatml', 'gemma2', 'gemma2_chatml', 'llama-3', 'llama3', 'phi-3', 'phi-35', 'phi-3.5', 'llama-3.1', 'llama-31', 'llama-3.2', 'llama-3.3', 'llama-32', 'llama-33', 'qwen-2.5', 'qwen-25', 'qwen25', 'qwen2.5', 'phi-4', 'gemma-3', 'gemma3']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "non_reasoning_dataset"
      ],
      "metadata": {
        "id": "tSAOEA4Pgq8U",
        "outputId": "efcb2d61-b9f0-4be4-f971-f40c211c744d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['instruction', 'input', 'output'],\n",
              "    num_rows: 3677727\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_chat_template(example):\n",
        "    # ÊûÑÈÄ†ÂØπËØùÊ†ºÂºè\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": f\"{example['instruction']}\\n{example['input']}\"},\n",
        "        {\"role\": \"assistant\", \"content\": example[\"output\"]}\n",
        "    ]\n",
        "    # Â∫îÁî® Qwen3 ÁöÑËÅäÂ§©Ê®°Êùø\n",
        "    example[\"text\"] = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "    return example\n",
        "\n",
        "# Â∫îÁî®Ê†ºÂºèÂåñ\n",
        "dataset = non_reasoning_dataset.map(format_chat_template)"
      ],
      "metadata": {
        "id": "hsP3pLM-cslF",
        "outputId": "0be15041-cc00-48f7-8226-3f6862d039b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c61aadd3eabc408686eebaac57b4cef4",
            "e6c3de0a9a0d4e80b7398bc07f5861e9",
            "f8eb3b9561da485ba45225b60e8e5955",
            "e03a111bab1644da9662826aefc2ebdf",
            "b3f71e4ac15c418fa43767a2747a2065",
            "2af71e2bfa45427397deff65b5089c23",
            "004e014ffbd24cb0ad36bb1e172b4f01",
            "e9d3b572926f47f391ee82a24dd03567",
            "956c9671b7f545c0a0e23360f896e4c2",
            "3e8a0a5445194e17a97e0032dc51e264",
            "2a53fd256a7b486bac5d0975e2bf2dca"
          ]
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3677727 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c61aadd3eabc408686eebaac57b4cef4"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "UnosJ46lhGhb",
        "outputId": "aef3a93f-86b1-4fcb-c25d-002b695485ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['instruction', 'input', 'output', 'text'],\n",
              "    num_rows: 3677727\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTexROzQfJn5"
      },
      "source": [
        "Let's see the first transformed row:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OMhyEXkfM5e"
      },
      "source": [
        "Next we take the non reasoning dataset and convert it to conversational format as well.\n",
        "\n",
        "We have to use Unsloth's `standardize_sharegpt` function to fix up the format of the dataset first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9FcosGvfdNr"
      },
      "source": [
        "Let's see the first row"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_0L18QMfot4"
      },
      "source": [
        "Now let's see how long both datasets are:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgknnOf7fn3e"
      },
      "source": [
        "The non reasoning dataset is much longer. Let's assume we want the model to retain some reasoning capabilities, but we specifically want a chat model.\n",
        "\n",
        "Let's define a ratio of chat only data. The goal is to define some mixture of both sets of data.\n",
        "\n",
        "Let's select 25% reasoning and 75% chat based:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_szfriCBgCkU"
      },
      "outputs": [],
      "source": [
        "chat_percentage = 0.95"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DANuEJA7gL58"
      },
      "source": [
        "Let's sample the reasoning dataset by 25% (or whatever is 100% - chat_percentage)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR-4prS_gVel"
      },
      "source": [
        "Finally combine both datasets:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "21823195c5d0410fb39c64afcda6c54c",
            "e18b2698947e4836a6bed5ffe792fd07",
            "bf138f67e2ad48f3a17ed4ebffbb3419",
            "e549f6ab45694142bfc584cfb6ff7f65",
            "d95e6041b0964215a145f5504f0e2469",
            "cc6ae42e08e0479693b7b5c0cfdb2882",
            "2d8674f2b6a94d0f85a79986cdf96bca",
            "f4ba93f081b74fcaae8f210bbb4f43a2",
            "dda981e2c0814abba85b4bca3458effe",
            "fa59e366abfc44a2b0c594c527bd7cb1",
            "4080fc8e5f774c48b405c04398f3fc8f"
          ]
        },
        "id": "95_Nn-89DhsL",
        "outputId": "a64eaacf-294a-455f-f180-09d05af1caf6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/3677727 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21823195c5d0410fb39c64afcda6c54c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    eval_dataset = None, # Can set up evaluation!\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 30,\n",
        "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "120f62aa-98fb-429c-9f9d-81fa03f25ae1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = Tesla T4. Max memory = 14.741 GB.\n",
            "11.898 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9fa371ShyhB"
      },
      "source": [
        "Let's train the model! To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "02b7087c-9b83-4360-8b69-62d4a75d8897"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 3,677,727 | Num Epochs = 1 | Total steps = 30\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 128,450,560/14,000,000,000 (0.92% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [30/30 04:44, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.248900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.163100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>4.634800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.776100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.989700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.258500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.643100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.538000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.675900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.145700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.419400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.992100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.975900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.853700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.297500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.940300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.774300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>2.160900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.661300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.791200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.912300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.236700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.504300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.498300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.829300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.571300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.376900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.285300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.755300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.498400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCqnaKmlO1U9",
        "outputId": "e93fcb6c-8d64-4a52-b96f-121297a8f31e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "302.9096 seconds used for training.\n",
            "5.05 minutes used for training.\n",
            "Peak reserved memory = 13.637 GB.\n",
            "Peak reserved memory for training = 1.739 GB.\n",
            "Peak reserved memory % of max memory = 92.511 %.\n",
            "Peak reserved memory for training % of max memory = 11.797 %.\n"
          ]
        }
      ],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model via Unsloth native inference! According to the `Qwen-3` team, the recommended settings for reasoning inference are `temperature = 0.6, top_p = 0.95, top_k = 20`\n",
        "\n",
        "For normal chat based inference, `temperature = 0.7, top_p = 0.8, top_k = 20`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR3gIAX-SM2q",
        "outputId": "449f9d08-8c43-41a6-f1b2-d6cfe29bb023"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Âπ≤Âí≥ÊúâÁó∞Áî®‰ªÄ‰πàËçØÔºüÂπ≤Âí≥ÊúâÁó∞ÂèØ‰ª•ÂêÉÔºöÂ∑ùË¥ùÈõ™Ê¢®ËÜèÔºåÊûáÊù∑ËÜèÔºåÊ≠¢Âí≥Á≥ñÊµÜÔºåÊ∏ÖËÇ∫ÂåñÁó∞‰∏∏ÔºåÂ§çÊñπÈ≤úÁ´πÊ≤•Ê∂≤ÔºåÂÖªÈò¥Ê∏ÖËÇ∫‰∏∏ÔºåËÇ∫ÂÆÅÈ¢óÁ≤íÔºåÂ§çÊñπÁîòËçâÁâáÔºåÊÄ•ÊîØÁ≥ñÊµÜÔºåÂçäÂ§èÊ≠¢Âí≥Á≥ñÊµÜÔºåÂ§çÊñπÊûáÊù∑Ê≠¢Âí≥ËÉ∂ÂõäÔºåËúúÁÇºÂ∑ùË¥ùÊûáÊù∑ËÜèÔºåÊ∏ÖËÇ∫Ê≠¢Âí≥‰∏∏ÔºåÊ∏ÖÂí≥Êï£ÔºåÊ≠¢Âí≥ÊûáÊù∑Á≥ñÊµÜÔºåÂº∫ÂäõÊûáÊù∑Èú≤ÔºåÊ∂¶ËÇ∫Ê≠¢Âí≥‰∏∏ÔºåÂ§çÊñπÂ∑ùË¥ùÊ≠¢Âí≥ËÜèÔºåÂ§çÊñπÊûáÊù∑Ê≠¢Âí≥Á≥ñÊµÜÔºåÊ≠¢Âí≥Âπ≥ÂñòÁ≥ñÊµÜÔºåÂ∑ùË¥ùÊûáÊù∑Á≥ñÊµÜÔºåÂ§çÊñπÁôæÈÉ®Ê≠¢Âí≥Á≥ñÊµÜÔºåÂ§çÊñπÈ≤úÁ´πÊ≤•Âè£ÊúçÊ∂≤ÔºåÊ∏ÖËÇ∫ÂåñÁó∞‰∏∏ÔºåÊ∏ÖËÇ∫Ê≠¢Âí≥‰∏∏ÔºåÊ∏ÖÂí≥Êï£ÔºåÊ≠¢Âí≥ÊûáÊù∑Á≥ñÊµÜÔºåÂº∫ÂäõÊûáÊù∑Èú≤ÔºåÊ∂¶ËÇ∫Ê≠¢Âí≥‰∏∏ÔºåÂ§çÊñπÂ∑ùË¥ùÊ≠¢Âí≥ËÜèÔºåÂ§çÊñπÊûáÊù∑Ê≠¢Âí≥Á≥ñÊµÜÔºåÊ≠¢Âí≥Âπ≥ÂñòÁ≥ñÊµÜÔºåÂ∑ùË¥ùÊûáÊù∑Á≥ñÊµÜÔºåÂ§çÊñπÁôæÈÉ®Ê≠¢Âí≥Á≥ñÊµÜÔºåÂ§çÊñπÈ≤úÁ´πÊ≤•Âè£ÊúçÊ∂≤ÔºåÊ∏ÖËÇ∫ÂåñÁó∞‰∏∏ÔºåÊ∏ÖËÇ∫Ê≠¢Âí≥‰∏∏\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\" : \"user\", \"content\" : \"Âπ≤Âí≥ÔºåÊúâÁó∞Áî®‰ªÄ‰πàËçØ\"}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    enable_thinking = False, # Disable thinking\n",
        ")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 256, # Increase for longer outputs!\n",
        "    temperature = 0.7, top_p = 0.8, top_k = 20, # For non thinking\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j873RMcEi9uq",
        "outputId": "e1f7c7b3-9225-4942-8227-606896ce30ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<think>\n",
            "\n",
            "</think>\n",
            "\n",
            "ÁñÆÁñ°Â∑≤Ê∫ÉÂêéÂá∫Áé∞ËôöÂº±Áä∂ÂÜµÁöÑÂéüÂõ†Ôºå‰∏ªË¶ÅÊòØÁî±‰∫éÈÇ™Ê∞îÊ∑±ÂÖ•‰ΩìÂÜÖÔºåÊ∂àËÄó‰∫ÜÊ≠£Ê∞î„ÄÇÂú®ÁñÆÁñ°Ê∫ÉÁÉÇÊó∂ÔºåËÑìÊ∂≤ÊµÅÂá∫Ôºå‰ºöÂ∏¶Ëµ∞‰ΩìÂÜÖÁöÑÁ≤æÊ∞îÔºåÂØºËá¥Ê≠£Ê∞îÂèóÊçüÔºåÂõ†Ê≠§ÊÇ£ËÄÖ‰ºöÊÑüÂà∞Ë∫´‰ΩìËôöÂº±„ÄÇ<|im_end|>\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\" : \"user\", \"content\" : \"ÁñÆÁñ°Â∑≤Ê∫ÉÂêéÔºå‰∏∫‰ªÄ‰πà‰ºöÂá∫Áé∞ËôöÂº±ÁöÑÁä∂ÂÜµÔºü\"}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    enable_thinking = True, # Disable thinking\n",
        ")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 1024, # Increase for longer outputs!\n",
        "    temperature = 0.6, top_p = 0.95, top_k = 20, # For thinking\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upcOlWe7A1vc",
        "outputId": "c2ddad72-3c41-498a-bed4-be3fe22269b5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('lora_model/tokenizer_config.json',\n",
              " 'lora_model/special_tokens_map.json',\n",
              " 'lora_model/vocab.json',\n",
              " 'lora_model/merges.txt',\n",
              " 'lora_model/added_tokens.json',\n",
              " 'lora_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "model.save_pretrained(\"lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"liocnn/Qwen3-14b-Chinese-Medicine\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"liocnn/Qwen3-14b-Chinese-Medicine\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "MKX_XKs_BNZR"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = 2048,\n",
        "        load_in_4bit = True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False:\n",
        "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False:\n",
        "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False:\n",
        "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
        "\n",
        "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False:\n",
        "    model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False:\n",
        "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False:\n",
        "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myFPzZ87EbRZ"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp or a UI based system like Jan or Open WebUI. You can install Jan [here](https://github.com/janhq/jan) and Open WebUI [here](https://github.com/open-webui/open-webui)\n",
        "\n",
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ‚≠êÔ∏è <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠êÔ∏è\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
